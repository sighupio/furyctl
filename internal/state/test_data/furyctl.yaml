# Copyright (c) 2017-present SIGHUP s.r.l All rights reserved.
# Use of this source code is governed by a BSD-style
# license that can be found in the LICENSE file.

apiVersion: kfd.sighup.io/v1alpha2
kind: EKSCluster
metadata:
  name: furyctl-dev-aws
spec:
  distributionVersion: "v1.24.1"
  toolsConfiguration:
    terraform:
      state:
        s3:
          bucketName: furyctl-test
          keyPrefix: furyctl/
          region: eu-west-1
  region: eu-west-1
  tags:
    env: "test"
    k8s: "awesome"
  infrastructure:
    vpc:
      network:
        cidr: 10.0.0.0/16
        subnetsCidrs:
          private:
            - 10.0.182.0/24
            - 10.0.172.0/24
            - 10.0.162.0/24
          public:
            - 10.0.20.0/24
            - 10.0.30.0/24
            - 10.0.40.0/24
    vpn:
      vpnClientsSubnetCidr: 192.168.200.0/24
      ssh:
        publicKeys:
          - "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIGxA+4CXYSmsN1m5DSsRzVRTegbvCh1gh7YObg0drj4+ alessio.pragliola@sighup.io"
        githubUsersName:
          - Al-Pragliola
        allowedFromCidrs:
          - 0.0.0.0/0
  kubernetes:
    nodeAllowedSshPublicKey: "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIGxA+4CXYSmsN1m5DSsRzVRTegbvCh1gh7YObg0drj4+ alessio.pragliola@sighup.io"
    nodePoolsLaunchKind: "launch_templates"
    nodePools:
      - name: worker
        ami:
          id: ami-0ab303329574a0338
          owner: "363601582189"
        size:
          min: 1
          max: 3
        subnetIds: []
        instance:
          type: t3.micro
          spot: false
          volumeSize: 50
        attachedTargetGroups: []
        labels:
          nodepool: worker
          node.kubernetes.io/role: worker
        taints:
          - node.kubernetes.io/role=worker:NoSchedule
        tags:
          k8s.io/cluster-autoscaler/node-template/label/nodepool: "worker"
          k8s.io/cluster-autoscaler/node-template/label/node.kubernetes.io/role: "worker"
      - name: worker-eks
        ami:
          id: ami-0ab303329574a0338
          owner: "363601582189"
        size:
          min: 1
          max: 3
        subnetIds: []
        instance:
          type: t3.micro
          spot: false
          volumeSize: 50
        attachedTargetGroups: []
        labels:
          nodepool: worker
          node.kubernetes.io/role: worker
        taints: []
        tags:
          k8s.io/cluster-autoscaler/node-template/label/nodepool: "worker"
          k8s.io/cluster-autoscaler/node-template/label/node.kubernetes.io/role: "worker"
    awsAuth:
      additionalAccounts: []
      users: []
      roles: []
  distribution:
    common:
      nodeSelector:
        node.kubernetes.io/role: infra
      tolerations:
        - effect: NoSchedule
          key: node.kubernetes.io/role
          value: infra
    modules:
      ingress:
        baseDomain: internal.fury-demo.sighup.io
        nginx:
          type: single
          tls:
            provider: certManager
            secret:
              cert: "{file://relative/path/to/ssl.crt}"
              key: "{file://relative/path/to/ssl.key}"
              ca: "{file://relative/path/to/ssl.ca}"
        certManager:
          clusterIssuer:
            name: letsencrypt-fury
            email: engineering+fury-distribution@sighup.io
            type: http01
        dns:
          public:
            name: "fury-demo.sighup.io"
            create: false
          private:
            create: true
            name: "internal.fury-demo.sighup.io"
            vpcId: "vpc123123123123"
      logging:
        overrides:
          nodeSelector: {}
          tolerations: []
          ingresses:
            opensearch-dashboards:
              disableAuth: false
              host: ""
              ingressClass: ""
            cerebro:
              disableAuth: false
              host: ""
              ingressClass: ""
        opensearch:
          type: single
          resources:
            requests:
              cpu: ""
              memory: ""
            limits:
              cpu: ""
              memory: ""
          storageSize: "150Gi"
      monitoring:
        overrides:
          nodeSelector: {}
          tolerations: []
          ingresses:
            prometheus:
              disableAuth: false
              host: ""
              ingressClass: ""
            alertmanager:
              disableAuth: false
              host: ""
              ingressClass: ""
            grafana:
              disableAuth: false
              host: ""
              ingressClass: ""
            goldpinger:
              disableAuth: false
              host: ""
              ingressClass: ""
        prometheus:
          resources:
            requests:
              cpu: ""
              memory: ""
            limits:
              cpu: ""
              memory: ""
      policy:
        overrides:
          nodeSelector: {}
          tolerations: []
          ingresses:
            gpm:
              disableAuth: false
              host: ""
              ingressClass: ""
        gatekeeper:
          additionalExcludedNamespaces: []
      dr:
        velero:
          eks:
            bucketName: example-velero
            iamRoleArn: arn:aws:iam::123456789012:role/example-velero # private.
            region: eu-west-1
      auth:
        provider:
          type: none
          basicAuth:
            username: admin
            password: "{env://KFD_BASIC_AUTH_PASSWORD}"
